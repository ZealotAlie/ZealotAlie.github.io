---
title: "DirectX 11 学习笔记-11【地形/粒子】"
tags: 图形学
      C++
---

相关代码已上传至[Github](https://github.com/ZealotAlie/D11Demo0)，具体来说，这部分的对应的Project是TerrainParticleCube。<!--more-->

项目运行的效果图如下，主要包括两部分内容，地形生成和简单的粒子效果。

![settings](/assets/images/2019-01-26-DX1111/1.jpg)

## 地形Terrain

生成地形需要两个纹理来构建地图信息，其一是高度图，它用于存储地形的高度信息，根据高度信息我们可以构建出地形网格；其二是BlendTexture，它用于存储地图各个位置的纹理信息，将Blendtexture上的颜色映射到对应的纹理贴图，根据颜色信息来用不同的贴图渲染地形。

### 创建

因为CPU也需要对高度信息进行存储和处理，所以Demo中直接把高度图信息存成二进制文件，每个像素点对应1byte。由于高度图的数据范围是0-255，所以读入数据之后，我们首先要将0-255映射到我们需要的高度范围。

```c++
void Terrain::LoadHeightmap()
{
	// A height for each vertex
	std::vector<unsigned char> in( mInfo.HeightmapWidth * mInfo.HeightmapHeight );

	// Open the file.
	std::ifstream inFile;
	inFile.open(mInfo.HeightMapFilename.c_str(), std::ios_base::binary);

	if(inFile)
	{
		// Read the RAW bytes.
		inFile.read((char*)&in[0], (std::streamsize)in.size());

		// Done with file.
		inFile.close();
	}

	// Copy the array data into a float array and scale it.
	mHeightmap.resize(mInfo.HeightmapHeight * mInfo.HeightmapWidth, 0);
	for(UINT i = 0; i < mInfo.HeightmapHeight * mInfo.HeightmapWidth; ++i)
	{
		mHeightmap[i] = (in[i] / 255.0f)*mInfo.HeightScale;
	}
}
```

读入数据之后，程序对数据做了平滑处理，因为我们的数据点是离散的网格，并且0-255的范围导致数据精确度不足，可能会出现一些较为尖锐的部分。平滑的函数实质就是模糊函数，取9个数据点的平均值，作为中心点的新值。

| | | |
|-|-|-|
|1|2|3|
|4|x|6|
|7|8|9|

```c++
void Terrain::Smooth()
{
	std::vector<float> dest( mHeightmap.size() );

	for(UINT i = 0; i < mInfo.HeightmapHeight; ++i)
	{
		for(UINT j = 0; j < mInfo.HeightmapWidth; ++j)
		{
			dest[i*mInfo.HeightmapWidth+j] = Average(i,j);
		}
	}

	// Replace the old heightmap with the filtered one.
	mHeightmap = dest;
}
```

模糊之后我们需要计算各个高度图网格点的高度范围，之所以需计算高度范围，是为了在曲面细分的时候判断网格位置是否可见，因为地形面积往往较大，有很大一部分是在视野之外的，通过剔除它们来提升性能。由于可以通过曲面细分来生成顶点，所以我们不需要直接生成大量的顶点，而是通过生成一个个Patch。在程序中，一个Patch表示的范围是64x64，这里的每个单位对应到高度图中的一个像素。因此计算每个Patch的高度范围需要考虑到内部的64x64格的高度。

```c++
void Terrain::CalcAllPatchBoundsY()
{
	mPatchBoundsY.resize(mNumPatchQuadFaces);

	// For each patch
	for(UINT i = 0; i < mNumPatchVertRows-1; ++i)
	{
		for(UINT j = 0; j < mNumPatchVertCols-1; ++j)
		{
			CalcPatchBoundsY(i, j);
		}
	}
}

void Terrain::CalcPatchBoundsY(UINT i, UINT j)
{
	// Scan the heightmap values this patch covers and compute the min/max height.

	UINT x0 = j*CellsPerPatch;
	UINT x1 = (j+1)*CellsPerPatch;

	UINT y0 = i*CellsPerPatch;
	UINT y1 = (i+1)*CellsPerPatch;

	float minY = +MathHelper::Infinity;
	float maxY = -MathHelper::Infinity;
	for(UINT y = y0; y <= y1; ++y)
	{
		for(UINT x = x0; x <= x1; ++x)
		{
			UINT k = y*mInfo.HeightmapWidth + x;
			minY = MathHelper::Min(minY, mHeightmap[k]);
			maxY = MathHelper::Max(maxY, mHeightmap[k]);
		}
	}

	UINT patchID = i*(mNumPatchVertCols-1)+j;
	mPatchBoundsY[patchID] = XMFLOAT2(minY, maxY);
}
```

计算完成后，我们需要把计算得到的数据传入到渲染管线之中，所以我们要创建相应的顶点缓冲区和纹理资源。

顶点缓冲区的创建和之前类似，由于我们会使用高度图中的高度信息，所以我们创建时不需要考虑y轴。前面提到我们每个patch对应64x64像素，所以Patch顶点的数量和高度图的数量是相关的，具体来说

```c++
// Divide heightmap into patches such that each patch has CellsPerPatch.
mNumPatchVertRows = ((mInfo.HeightmapHeight-1) / CellsPerPatch) + 1;
mNumPatchVertCols = ((mInfo.HeightmapWidth-1) / CellsPerPatch) + 1;
```

Terrain的顶点定义如下，

```c++
struct TerrainVertex
{
	XMFLOAT3 Pos;
	XMFLOAT2 Tex;
	XMFLOAT2 BoundsY;
};
```

其中Pos是Patch顶点的空间位置，Tex是该位置对应到高度图中的UV坐标，BoundsY则是我们刚刚计算出来的Patch的最大和最小Y值。

创建好顶点缓冲区后，我们还需要为高度图创建纹理资源。

```c++
void Terrain::BuildHeightmapSRV(ID3D11Device* device)
{
	D3D11_TEXTURE2D_DESC texDesc;
	texDesc.Width = mInfo.HeightmapWidth;
	texDesc.Height = mInfo.HeightmapHeight;
    texDesc.MipLevels = 1;
	texDesc.ArraySize = 1;
	texDesc.Format    = DXGI_FORMAT_R16_FLOAT;
	texDesc.SampleDesc.Count   = 1;
	texDesc.SampleDesc.Quality = 0;
	texDesc.Usage = D3D11_USAGE_DEFAULT;
	texDesc.BindFlags = D3D11_BIND_SHADER_RESOURCE;
	texDesc.CPUAccessFlags = 0;
	texDesc.MiscFlags = 0;

	// HALF is defined in xnamath.h, for storing 16-bit float.
	std::vector<HALF> hmap(mHeightmap.size());
	std::transform(mHeightmap.begin(), mHeightmap.end(), hmap.begin(),XMConvertFloatToHalf);
	
	D3D11_SUBRESOURCE_DATA data;
	data.pSysMem = &hmap[0];
    data.SysMemPitch = mInfo.HeightmapWidth*sizeof(HALF);
    data.SysMemSlicePitch = 0;

	ID3D11Texture2D* hmapTex = 0;
	HR(device->CreateTexture2D(&texDesc, &data, &hmapTex));

	D3D11_SHADER_RESOURCE_VIEW_DESC srvDesc;
	srvDesc.Format = texDesc.Format;
	srvDesc.ViewDimension = D3D11_SRV_DIMENSION_TEXTURE2D;
	srvDesc.Texture2D.MostDetailedMip = 0;
	srvDesc.Texture2D.MipLevels = -1;
	HR(device->CreateShaderResourceView(hmapTex, &srvDesc, &mHeightMapSRV));

	// SRV saves reference.
	ReleaseCOM(hmapTex);
}
```

因为我们只需要存放一个数（只有R，没有GBA）在纹理中，所以这里使用的格式是DXGI_FORMAT_R16_FLOAT。我们不需要Mipmap，所以MipLevel设置为1。

在填入D3D11_SUBRESOURCE_DATA 时，用到了SysMemPitch参数，这个参数用于指定每行数据的长度，以byte为单位，这里的值为HeightmapWidth*sizeof(HALF)。

### 渲染

渲染时，我们按照管线的顺序来分析这部分。

#### **Vertex Shader**

```glsl
VertexOut VS(VertexIn vin)
{
	VertexOut vout;
	
	// Terrain specified directly in world space.
	vout.PosW = vin.PosL;

	// Displace the patch corners to world space.  This is to make 
	// the eye to patch distance calculation more accurate.
	vout.PosW.y = gHeightMap.SampleLevel( samHeightmap, vin.Tex, 0 ).r;

	// Output vertex attributes to next stage.
	vout.Tex      = vin.Tex;
	vout.BoundsY  = vin.BoundsY;
	
	return vout;
}
```

我们的主要工作会在DS中完成，所以VS不需要做太多的事情。这里对高度图进行采样并将高度赋值到了顶点中，因为在曲面细分时要计算Patch到摄像机的距离，所以需要完整的顶点位置信息。

#### **Hull Shader**

HS中主要看ConstantHS。

```c++
PatchTess ConstantHS(InputPatch<VertexOut, 4> patch, uint patchID : SV_PrimitiveID)
{
	PatchTess pt;
	
	//
	// Frustum cull
	//
	
	// We store the patch BoundsY in the first control point.
	float minY = patch[0].BoundsY.x;
	float maxY = patch[0].BoundsY.y;
	
	// Build axis-aligned bounding box.  patch[2] is lower-left corner
	// and patch[1] is upper-right corner.
	float3 vMin = float3(patch[2].PosW.x, minY, patch[2].PosW.z);
	float3 vMax = float3(patch[1].PosW.x, maxY, patch[1].PosW.z);
	
	float3 boxCenter  = 0.5f*(vMin + vMax);
	float3 boxExtents = 0.5f*(vMax - vMin);
	if( AabbOutsideFrustumTest(boxCenter, boxExtents, gWorldFrustumPlanes) )
	{
		pt.EdgeTess[0] = 0.0f;
		pt.EdgeTess[1] = 0.0f;
		pt.EdgeTess[2] = 0.0f;
		pt.EdgeTess[3] = 0.0f;
		
		pt.InsideTess[0] = 0.0f;
		pt.InsideTess[1] = 0.0f;
		
		return pt;
	}
	//
	// Do normal tessellation based on distance.
	//
	else 
	{
		// It is important to do the tess factor calculation based on the
		// edge properties so that edges shared by more than one patch will
		// have the same tessellation factor.  Otherwise, gaps can appear.
		
		// Compute midpoint on edges, and patch center
		float3 e0 = 0.5f*(patch[0].PosW + patch[2].PosW);
		float3 e1 = 0.5f*(patch[0].PosW + patch[1].PosW);
		float3 e2 = 0.5f*(patch[1].PosW + patch[3].PosW);
		float3 e3 = 0.5f*(patch[2].PosW + patch[3].PosW);
		float3  c = 0.25f*(patch[0].PosW + patch[1].PosW + patch[2].PosW + patch[3].PosW);
		
		pt.EdgeTess[0] = CalcTessFactor(e0);
		pt.EdgeTess[1] = CalcTessFactor(e1);
		pt.EdgeTess[2] = CalcTessFactor(e2);
		pt.EdgeTess[3] = CalcTessFactor(e3);
		
		pt.InsideTess[0] = CalcTessFactor(c);
		pt.InsideTess[1] = pt.InsideTess[0];
	
		return pt;
	}
}
```

它的主要作用是根据Patch和摄像机的位置来确定细分参数，越近的Patch细分的图形越多，图像越精细。另外它还做了一个可见性的判断，如果整个Patch都位于视锥之外，则丢弃掉这个Patch（细分参数为0的Patch会被丢弃）。

由于会有两个图形共享同一条边，所以我们要确保在两个平面图形中，这条边的细分参数是一致的，否则可能出现缺口。因此在计算距离时，边的距离为边中点到摄像机距离，而内部的tessellation factor则通过图形的中心点到摄像机的距离来计算。

#### **Domain Shader**

```glsl
[domain("quad")]
DomainOut DS(PatchTess patchTess, 
             float2 uv : SV_DomainLocation, 
             const OutputPatch<HullOut, 4> quad)
{
	DomainOut dout;
	
	// Bilinear interpolation.
	dout.PosW = lerp(
		lerp(quad[0].PosW, quad[1].PosW, uv.x),
		lerp(quad[2].PosW, quad[3].PosW, uv.x),
		uv.y); 
	
	dout.Tex = lerp(
		lerp(quad[0].Tex, quad[1].Tex, uv.x),
		lerp(quad[2].Tex, quad[3].Tex, uv.x),
		uv.y); 
		
	// Tile layer textures over terrain.
	dout.TiledTex = dout.Tex*gTexScale; 
	
	// Displacement mapping
	dout.PosW.y = gHeightMap.SampleLevel( samHeightmap, dout.Tex, 0 ).r;
	
	// NOTE: We tried computing the normal in the shader using finite difference, 
	// but the vertices move continuously with fractional_even which creates
	// noticable light shimmering artifacts as the normal changes.  Therefore,
	// we moved the calculation to the pixel shader.  
	
	// Project to homogeneous clip space.
	dout.PosH    = mul(float4(dout.PosW, 1.0f), gViewProj);
	
	return dout;
}
```

DS的主要功能是通过插值得出输出点的x,z坐标信息，通过采样高度图获得y值，最后再将坐标点转换到齐次裁剪坐标空间中。

#### **Pixel Shader**

```c++
float4 PS(DomainOut pin, 
          uniform int gLightCount, 
		  uniform bool gFogEnabled) : SV_Target
{
	//
	// Estimate normal and tangent using central differences.
	//
	float2 leftTex   = pin.Tex + float2(-gTexelCellSpaceU, 0.0f);
	float2 rightTex  = pin.Tex + float2(gTexelCellSpaceU, 0.0f);
	float2 bottomTex = pin.Tex + float2(0.0f, gTexelCellSpaceV);
	float2 topTex    = pin.Tex + float2(0.0f, -gTexelCellSpaceV);
	
	float leftY   = gHeightMap.SampleLevel( samHeightmap, leftTex, 0 ).r;
	float rightY  = gHeightMap.SampleLevel( samHeightmap, rightTex, 0 ).r;
	float bottomY = gHeightMap.SampleLevel( samHeightmap, bottomTex, 0 ).r;
	float topY    = gHeightMap.SampleLevel( samHeightmap, topTex, 0 ).r;
	
	float3 tangent = normalize(float3(2.0f*gWorldCellSpace, rightY - leftY, 0.0f));
	float3 bitan   = normalize(float3(0.0f, bottomY - topY, -2.0f*gWorldCellSpace)); 
	float3 normalW = cross(tangent, bitan);


	// The toEye vector is used in lighting.
	float3 toEye = gEyePosW - pin.PosW;

	// Cache the distance to the eye from this surface point.
	float distToEye = length(toEye);

	// Normalize.
	toEye /= distToEye;
	
	//
	// Texturing
	//
	
	// Sample layers in texture array.
	float4 c0 = gLayerMapArray.Sample( samLinear, float3(pin.TiledTex, 0.0f) );
	float4 c1 = gLayerMapArray.Sample( samLinear, float3(pin.TiledTex, 1.0f) );
	float4 c2 = gLayerMapArray.Sample( samLinear, float3(pin.TiledTex, 2.0f) );
	float4 c3 = gLayerMapArray.Sample( samLinear, float3(pin.TiledTex, 3.0f) );
	float4 c4 = gLayerMapArray.Sample( samLinear, float3(pin.TiledTex, 4.0f) ); 
	
	// Sample the blend map.
	float4 t  = gBlendMap.Sample( samLinear, pin.Tex ); 
    
    // Blend the layers on top of each other.
    float4 texColor = c0;
    texColor = lerp(texColor, c1, t.r);
    texColor = lerp(texColor, c2, t.g);
    texColor = lerp(texColor, c3, t.b);
    texColor = lerp(texColor, c4, t.a);

... ...

    return litColor;
}
```

在PS中，首先我们需要计算法线。这里首先计算出x和y两个方向的切线，通过叉乘即可获得法线。

之后我们需要获得像素点的顶点纹理信息。根据blend纹理，将我们需要的纹理混合在一起。一张纹理有rgba四个量，最后一个分量可以通过1-r-g-b-a获得，所以最大支持五张贴图的混合。使用lerp混合之后获得地形的颜色，之后再正常计算光照/雾等效果即可。

## 粒子系统

这一部分由于不一样的粒子效果需要不同的Shader，所以不详细讲解代码，只对整体流程做一个简单介绍。

### 随机数

在粒子系统中经常需要使用到随机数功能。在HLSL中没有内置的随机数函数，所以我们需要添加自己的随机数功能。

实现起来比较简单，只需要通过随机函数生成一组随机数，然后将它们存成纹理格式储存在缓存中。通过对随机纹理采样即可获得随机数。

### 混合

在粒子系统中往往需要使用到混合，其中最为常见的是AdditiveBlending，即将src和dest的颜色直接相加。

```
SrcBlend    = SRC_ALPHA;
DestBlend   = ONE;
BlendOp     = ADD;
```

有些情况下也使用

```
SrcBlend    = ONE;
DestBlend   = ONE;
BlendOp     = ADD;
```

此时Alpha分量在计算颜色时不被考虑。

### Stream-Out

GPU粒子系统分为两部分，一部分是生成粒子的信息，主要是transform信息，另一部分是根据这些信息绘制粒子。生成粒子需要GPU输出顶点信息，而绘制则需要将顶点信息传入，因为同一个buffer不能即作为输入也作为输出，所以这里最起码需要两个顶点缓冲区。

实现上这里有个技巧，因为两个buffer的大小应该是一致的，存放的内容也十分相似，所以我们只创建两个缓冲区，交替使用它们

![StreamOut](/assets/images/2019-01-26-DX1111/2.jpg)

在创建缓冲区时，因为缓冲区既要作为顶点缓冲区，也要作为Stream-Out的缓冲区，所以创建时要指定相应的tag

![StreamOut](/assets/images/2019-01-26-DX1111/3.jpg)

通过调用SOSetTargets可以把缓冲区绑定到SO输出阶段。由于CPU不知道SO输出的顶点的数量，所以在绘制顶点时不能使用Draw，而应该使用DrawAuto。

在StreamOut时，通过设置SetPixelShader(NULL);可以让管线只从OS输出信息而不会渲染像素。

### GPU粒子系统

具体到特定的粒子效果，需要使用特定的shader来实现，不过可以在CPP代码中抽象出一个粒子系统。

粒子系统除了随机数之外，往往还需要获取游戏时间/TimeStep等信息。

在StreamOut中，需要完成两部分工作，其一是生成新的粒子，其二是将过期的粒子删除，这部分主要通过判断粒子的Age来实现。在粒子顶点中一个字段用来判断粒子是发射器还是生成的粒子，在每次GS中，顶点的Age加上TimeStep。对发射器而言，这个Age可以作为生成新粒子的计时器，每经过dt时间就生成一批新的粒子并添加到输出顶点中。而对于粒子顶点，这个计时器作为粒子的生命计数器，当粒子的Age大于某值是不输出它，那么它就被移除了。

## 参考资料

- 《Introduction to 3D Game Programming with Directx 11》